{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F-dHYfDVhEu",
        "outputId": "08f09a41-21fd-4ddb-a0ab-76ddc7385b0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet pandas numpy scikit-learn matplotlib seaborn tqdm catboost tqdm-joblib\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjy95YFB5uoJ",
        "outputId": "a6e3c6f9-08f3-415c-fb86-e5541b6c4e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.8.1-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from category_encoders) (0.14.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Downloading category_encoders-2.8.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet catboost pandas numpy scikit-learn tqdm imbalanced-learn\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, StratifiedKFold, RandomizedSearchCV\n",
        ")\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, make_scorer\n",
        "from catboost import CatBoostClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# --- 설정 및 경로 정의 ---\n",
        "# 프로젝트 루트\n",
        "base_dir = '/content/drive/MyDrive/Colab Notebooks/패턴인식'\n",
        "data_path  = os.path.join(base_dir, 'data_preprocessing/result', 'trial1_train.csv')\n",
        "log_root   = os.path.join(base_dir, 'catboost', 'log')\n",
        "timestamp  = time.strftime('%Y%m%d_%H%M%S')\n",
        "output_dir = os.path.join(log_root, f'trial_derived_features_{timestamp}')\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "MODEL_FILE = os.path.join(output_dir, 'catboost_final_model_cpu_derived.cbm')\n",
        "LOG_FILE   = os.path.join(output_dir, 'log_derived_features.txt')\n",
        "FI_CSV     = os.path.join(output_dir, 'feature_importances_derived.csv')\n",
        "PI_CSV     = os.path.join(output_dir, 'permutation_importances_derived.csv')\n",
        "\n",
        "N_ITER_SEARCH = 30  # RandomizedSearchCV 반복 횟수\n",
        "\n",
        "# --- 1) 데이터 로드 ---\n",
        "start_time = time.time()\n",
        "print(\"1) Loading data...\")\n",
        "df = pd.read_csv(data_path)\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "# --- 2) 파생 변수 생성 ---\n",
        "print(\"\\n2) Creating ONLY the previously useful derived features...\")\n",
        "epsilon = 1e-6\n",
        "if 'n_tokens_content' in X and 'num_imgs' in X:\n",
        "    X['feat_content_to_img_ratio'] = X['n_tokens_content'] / (X['num_imgs'] + epsilon)\n",
        "if 'global_subjectivity' in X and 'global_sentiment_polarity' in X:\n",
        "    X['feat_global_sentiment_strength'] = X['global_subjectivity'] * X['global_sentiment_polarity']\n",
        "if 'n_tokens_content' in X and 'num_hrefs' in X:\n",
        "    X['feat_content_to_href_ratio'] = X['n_tokens_content'] / (X['num_hrefs'] + epsilon)\n",
        "print(\"Only previously useful derived features created.\")\n",
        "\n",
        "# --- 3) Train/Test 분할 ---\n",
        "print(\"\\n3) Splitting data into training and test sets...\")\n",
        "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    stratify=y,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- 4) 수치형 결측치 처리 (중위수 대체) ---\n",
        "print(\"\\n4) Applying preprocessing (missing value imputation)...\")\n",
        "num_cols = X_trainval.select_dtypes(include=[np.number]).columns.tolist()\n",
        "medi = X_trainval[num_cols].median()\n",
        "X_trainval[num_cols] = X_trainval[num_cols].fillna(medi)\n",
        "X_test[num_cols]      = X_test[num_cols].fillna(medi)\n",
        "\n",
        "# --- 4.1) 원핫 복원 → 타깃 인코딩 ---\n",
        "print(\"\\n4.1) One-hot restore & Target encoding...\")\n",
        "global_mean = y_trainval.mean()\n",
        "# data_channel 복원\n",
        "data_channel_cols = [c for c in X_trainval if c.startswith(\"data_channel_\")]\n",
        "weekday_cols      = [c for c in X_trainval if c.startswith(\"weekday_\")]\n",
        "\n",
        "def restore_from_onehot(df, cols, prefix):\n",
        "    return df[cols].idxmax(axis=1).str.replace(f\"{prefix}_\", \"\", regex=False)\n",
        "\n",
        "if data_channel_cols:\n",
        "    X_trainval['data_channel'] = restore_from_onehot(X_trainval, data_channel_cols, \"data_channel\")\n",
        "    X_test['data_channel']     = restore_from_onehot(X_test, data_channel_cols, \"data_channel\")\n",
        "if weekday_cols:\n",
        "    X_trainval['weekday'] = restore_from_onehot(X_trainval, weekday_cols, \"weekday\")\n",
        "    X_test['weekday']     = restore_from_onehot(X_test, weekday_cols, \"weekday\")\n",
        "\n",
        "for col in ['data_channel', 'weekday']:\n",
        "    if col in X_trainval:\n",
        "        means = X_trainval.join(y_trainval).groupby(col)[y.name].mean()\n",
        "        X_trainval[col] = X_trainval[col].map(means)\n",
        "        X_test[col]     = X_test[col].map(means).fillna(global_mean)\n",
        "\n",
        "# --- 4.5) SMOTE 적용 ---\n",
        "print(\"\\n4.5) Adjusting for class imbalance with SMOTE...\")\n",
        "smote = SMOTE(random_state=42)\n",
        "X_trainval, y_trainval = smote.fit_resample(X_trainval, y_trainval)\n",
        "print(f\"After SMOTE → TrainVal: {X_trainval.shape}, class counts: {np.bincount(y_trainval)}\")\n",
        "\n",
        "# --- 5) 하이퍼파라미터 탐색 범위 정의 ---\n",
        "print(\"\\n5) Defining hyperparameter search space...\")\n",
        "param_dist_tuned = {\n",
        "    'learning_rate':      [0.01, 0.03, 0.05, 0.07, 0.1],\n",
        "    'depth':              [4, 6, 8, 10],\n",
        "    'l2_leaf_reg':        [1, 3, 5, 7, 9, 12],\n",
        "    'border_count':       [32, 64, 128, 254],\n",
        "    'bagging_temperature':[0, 0.5, 1.0, 1.5, 2.0],\n",
        "    'random_strength':    [0.1, 0.5, 1, 2, 5],\n",
        "    'colsample_bylevel':  [0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# --- 6) RandomizedSearchCV 세팅 및 실행 ---\n",
        "print(f\"\\n6) Starting RandomizedSearchCV ({N_ITER_SEARCH} iterations) using CPU...\")\n",
        "base_model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        "    task_type='GPU',\n",
        "    devices='0'\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=base_model,\n",
        "    param_distributions=param_dist_tuned,\n",
        "    n_iter=N_ITER_SEARCH,\n",
        "    scoring='roc_auc',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "search.fit(\n",
        "    X_trainval,\n",
        "    y_trainval,\n",
        "    cat_features=None  # 필요한 경우 cat_cols 리스트를 지정\n",
        ")\n",
        "\n",
        "best_params = search.best_params_\n",
        "best_score  = search.best_score_\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RandomizedSearchCV Results:\")\n",
        "print(\"=\"*50)\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "print(f\"Best CV ROC AUC: {best_score:.4f}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# --- 7) 최종 모델 재학습 (Early Stopping) ---\n",
        "print(\"\\n7) Training final tuned model with early stopping on test set...\")\n",
        "final_model = CatBoostClassifier(\n",
        "    iterations=2000,\n",
        "    eval_metric='AUC',\n",
        "    early_stopping_rounds=50,\n",
        "    use_best_model=True,\n",
        "    random_state=42,\n",
        "    verbose=100,\n",
        "    task_type='GPU',\n",
        "    devices='0',\n",
        "    **best_params\n",
        ")\n",
        "\n",
        "final_model.fit(\n",
        "    X_trainval, y_trainval,\n",
        "    cat_features=None,\n",
        "    eval_set=[(X_test, y_test)]\n",
        ")\n",
        "best_iter = final_model.get_best_iteration()\n",
        "print(f\"Best iteration: {best_iter}\")\n",
        "\n",
        "# feature importance, permutation importance, 평가, 저장\n",
        "fi = final_model.get_feature_importance(prettified=True)\n",
        "fi.head(20).to_csv(FI_CSV, index=False)\n",
        "print(f\"▶ Feature importances saved to {FI_CSV}\")\n",
        "\n",
        "y_pred   = final_model.predict(X_test)\n",
        "y_prob   = final_model.predict_proba(X_test)[:, 1]\n",
        "acc_test = accuracy_score(y_test, y_pred)\n",
        "f1_test  = f1_score(y_test, y_pred)\n",
        "auc_test = roc_auc_score(y_test, y_prob)\n",
        "comp_test= (acc_test + f1_test + auc_test) / 3\n",
        "\n",
        "print(\"▶ Final hold-out performance:\")\n",
        "print(f\"   Accuracy : {acc_test:.4f}\")\n",
        "print(f\"   F1 Score : {f1_test:.4f}\")\n",
        "print(f\"   ROC AUC  : {auc_test:.4f}\")\n",
        "print(f\"   Composite: {comp_test:.4f}\")\n",
        "\n",
        "final_model.save_model(MODEL_FILE)\n",
        "print(f\"▶ Model saved to {MODEL_FILE}\")\n",
        "\n",
        "with open(LOG_FILE, 'w') as f:\n",
        "    f.write(f\"Best params      : {best_params}\\n\")\n",
        "    f.write(f\"Best CV ROC AUC  : {best_score:.4f}\\n\")\n",
        "    f.write(f\"Best iteration   : {best_iter}\\n\")\n",
        "    f.write(f\"Test Accuracy    : {acc_test:.4f}\\n\")\n",
        "    f.write(f\"Test F1 Score    : {f1_test:.4f}\\n\")\n",
        "    f.write(f\"Test ROC AUC     : {auc_test:.4f}\\n\")\n",
        "    f.write(f\"Test Composite   : {comp_test:.4f}\\n\")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed  = end_time - start_time\n",
        "hrs      = int(elapsed // 3600)\n",
        "mins     = int((elapsed % 3600) // 60)\n",
        "secs     = int(elapsed % 60)\n",
        "print(f\"▶ Total execution time: {hrs}h {mins}m {secs}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUV-KXC5xsr7",
        "outputId": "fb2cbd88-ed21-4b50-ec6b-8a3172bacc65"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1) Loading data...\n",
            "\n",
            "2) Creating ONLY the previously useful derived features...\n",
            "Only previously useful derived features created.\n",
            "\n",
            "3) Splitting data into training and test sets...\n",
            "\n",
            "4) Applying preprocessing (missing value imputation)...\n",
            "\n",
            "4.1) One-hot restore & Target encoding...\n",
            "\n",
            "4.5) Adjusting for class imbalance with SMOTE...\n",
            "After SMOTE → TrainVal: (17914, 62), class counts: [8957 8957]\n",
            "\n",
            "5) Defining hyperparameter search space...\n",
            "\n",
            "6) Starting RandomizedSearchCV (30 iterations) using CPU...\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "120 fits failed out of a total of 150.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "120 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 5245, in fit\n",
            "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 2395, in _fit\n",
            "    train_params = self._prepare_train_params(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 2321, in _prepare_train_params\n",
            "    _check_train_params(params)\n",
            "  File \"_catboost.pyx\", line 6601, in _catboost._check_train_params\n",
            "  File \"_catboost.pyx\", line 6623, in _catboost._check_train_params\n",
            "_catboost.CatBoostError: catboost/private/libs/options/catboost_options.cpp:637: Error: rsm on GPU is supported for pairwise modes only\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "RandomizedSearchCV Results:\n",
            "==================================================\n",
            "Best hyperparameters: {'random_strength': 1, 'learning_rate': 0.03, 'l2_leaf_reg': 3, 'depth': 8, 'colsample_bylevel': 1.0, 'border_count': 128, 'bagging_temperature': 1.5}\n",
            "Best CV ROC AUC: 0.7200\n",
            "==================================================\n",
            "\n",
            "7) Training final tuned model with early stopping on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\ttest: 0.6794876\tbest: 0.6794876 (0)\ttotal: 104ms\tremaining: 3m 27s\n",
            "100:\ttest: 0.7220915\tbest: 0.7221278 (99)\ttotal: 10.8s\tremaining: 3m 22s\n",
            "200:\ttest: 0.7256352\tbest: 0.7256352 (200)\ttotal: 20.7s\tremaining: 3m 5s\n",
            "300:\ttest: 0.7273985\tbest: 0.7275464 (297)\ttotal: 30.5s\tremaining: 2m 52s\n",
            "400:\ttest: 0.7279806\tbest: 0.7280679 (369)\ttotal: 41s\tremaining: 2m 43s\n",
            "500:\ttest: 0.7278011\tbest: 0.7283238 (468)\ttotal: 51.6s\tremaining: 2m 34s\n",
            "bestTest = 0.7283237576\n",
            "bestIteration = 468\n",
            "Shrink model to first 469 iterations.\n",
            "Best iteration: 468\n",
            "▶ Feature importances saved to /content/drive/MyDrive/Colab Notebooks/패턴인식/catboost/log/trial_derived_features_20250528_120658/feature_importances_derived.csv\n",
            "▶ Final hold-out performance:\n",
            "   Accuracy : 0.6662\n",
            "   F1 Score : 0.6643\n",
            "   ROC AUC  : 0.7283\n",
            "   Composite: 0.6863\n",
            "▶ Model saved to /content/drive/MyDrive/Colab Notebooks/패턴인식/catboost/log/trial_derived_features_20250528_120658/catboost_final_model_cpu_derived.cbm\n",
            "▶ Total execution time: 0h 40m 12s\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}